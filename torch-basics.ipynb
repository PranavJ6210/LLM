{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are analysing how long the GPU takes to create this random zero valued torch across 1000 x 1000 matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.0010025501\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "#matrix operations here\n",
    "zeros = torch.zeros(1000,1000)\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"{elapsed_time: .10f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print(zeros)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are comparing the performace of Numpy CPU vs Torch GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.11052132\n",
      " 0.06918025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harid\\AppData\\Local\\Temp\\ipykernel_15096\\2410692463.py:18: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  randN = np.multiply(np_rand1,np_rand2)\n"
     ]
    }
   ],
   "source": [
    "torch_rand1 =  torch.rand(10000,10000).to(device)\n",
    "torch_rand2 =  torch.rand(10000,10000).to(device)\n",
    "np_rand1 = torch.rand(10000,10000)\n",
    "np_rand2 = torch.rand(10000,10000)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "randT = (torch_rand1 @ torch_rand2)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"{elapsed_time: .8f}\")\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "randN = np.multiply(np_rand1,np_rand2)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"{elapsed_time: .8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for a simple 2 x 2 matrix, the CPU beats the GPU, but if the case is slightly different like..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 5.96 GiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Of the allocated memory 6.71 GiB is allocated by PyTorch, and 11.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m torch_rand1 \u001b[38;5;241m=\u001b[39m  torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m200\u001b[39m,\u001b[38;5;241m200\u001b[39m,\u001b[38;5;241m200\u001b[39m,\u001b[38;5;241m200\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 2\u001b[0m torch_rand2 \u001b[38;5;241m=\u001b[39m  \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m np_rand1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m200\u001b[39m,\u001b[38;5;241m200\u001b[39m,\u001b[38;5;241m200\u001b[39m,\u001b[38;5;241m200\u001b[39m)\n\u001b[0;32m      4\u001b[0m np_rand2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m200\u001b[39m,\u001b[38;5;241m200\u001b[39m,\u001b[38;5;241m200\u001b[39m,\u001b[38;5;241m200\u001b[39m)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 5.96 GiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Of the allocated memory 6.71 GiB is allocated by PyTorch, and 11.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "torch_rand1 =  torch.rand(200,200,200,200).to(device)\n",
    "torch_rand2 =  torch.rand(200,200,200,200).to(device)\n",
    "np_rand1 = torch.rand(200,200,200,200)\n",
    "np_rand2 = torch.rand(200,200,200,200)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "randT = (torch_rand1 @ torch_rand2)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"GPU time taken :{elapsed_time: .8f}\")\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "randN = np.multiply(np_rand1,np_rand2)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"CPU time taken :{elapsed_time: .8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well this was to show the limitation of my GPU and learn a little about it, you can also try absurd methods like this. It took well over 20 seconds to print this by the way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU time taken : 0.02585101\n",
      "CPU time taken : 0.12843204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harid\\AppData\\Local\\Temp\\ipykernel_15096\\4197572064.py:18: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  randN = np.multiply(np_rand1,np_rand2)\n"
     ]
    }
   ],
   "source": [
    "torch_rand1 =  torch.rand(110,110,110,110).to(device)\n",
    "torch_rand2 =  torch.rand(110,110,110,110).to(device)\n",
    "np_rand1 = torch.rand(110,110,110,110)\n",
    "np_rand2 = torch.rand(110,110,110,110)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "randT = (torch_rand1 @ torch_rand2)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"GPU time taken :{elapsed_time: .8f}\")\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "randN = np.multiply(np_rand1,np_rand2)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"CPU time taken :{elapsed_time: .8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in the above case the GPU was much faster than the CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-venv",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
